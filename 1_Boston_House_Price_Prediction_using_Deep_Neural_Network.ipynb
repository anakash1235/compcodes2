{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NwcENUehsvYm",
    "outputId": "f44393fb-c20e-4259-c92d-2ec819099195"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:301: UserWarning: Multiple active versions of the dataset matching the name boston exist. Versions may be fundamentally different, returning version 1.\n",
      "  warn(\n",
      "C:\\Python311\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS CHAS    NOX     RM    AGE     DIS RAD    TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31    0  0.538  6.575   65.2  4.0900   1  296.0     15.3   \n",
       "1  0.02731   0.0   7.07    0  0.469  6.421   78.9  4.9671   2  242.0     17.8   \n",
       "2  0.02729   0.0   7.07    0  0.469  7.185   61.1  4.9671   2  242.0     17.8   \n",
       "3  0.03237   0.0   2.18    0  0.458  6.998   45.8  6.0622   3  222.0     18.7   \n",
       "4  0.06905   0.0   2.18    0  0.458  7.147   54.2  6.0622   3  222.0     18.7   \n",
       "5  0.02985   0.0   2.18    0  0.458  6.430   58.7  6.0622   3  222.0     18.7   \n",
       "6  0.08829  12.5   7.87    0  0.524  6.012   66.6  5.5605   5  311.0     15.2   \n",
       "7  0.14455  12.5   7.87    0  0.524  6.172   96.1  5.9505   5  311.0     15.2   \n",
       "8  0.21124  12.5   7.87    0  0.524  5.631  100.0  6.0821   5  311.0     15.2   \n",
       "9  0.17004  12.5   7.87    0  0.524  6.004   85.9  6.5921   5  311.0     15.2   \n",
       "\n",
       "        B  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  \n",
       "5  394.12   5.21  28.7  \n",
       "6  395.60  12.43  22.9  \n",
       "7  396.90  19.15  27.1  \n",
       "8  386.63  29.93  16.5  \n",
       "9  386.71  17.10  18.9  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "import pandas as pd\n",
    "\n",
    "boston_dataset = fetch_openml(name='boston')#load_boston()\n",
    "\n",
    "df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "df['MEDV'] = boston_dataset.target\n",
    "\n",
    "df.head(n=10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4FEDjg8rsyi0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.loc[:, df.columns != 'MEDV']\n",
    "y = df.loc[:, df.columns == 'MEDV']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(X_train)\n",
    "X_train = mms.transform(X_train)\n",
    "X_test = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VL8VMy_fs3fl",
    "outputId": "45cccbbb-2771-4a26-edfc-b0a66c875ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 128)               1792      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# If this does not work append \"tensorflow.\" before keras.\n",
    "# example: tensorflow.keras.models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=(13, ), activation='relu', name='dense_1'))\n",
    "model.add(Dense(64, activation='relu', name='dense_2'))\n",
    "model.add(Dense(1, activation='linear', name='dense_output'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7m71ooKs5of",
    "outputId": "06852ed4-2013-488c-8707-6826dd1d4412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 7.0257 - mae: 1.9057 - val_loss: 4.8119 - val_mae: 1.8133\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.6116 - mae: 1.8677 - val_loss: 4.3839 - val_mae: 1.7087\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.5825 - mae: 1.8914 - val_loss: 4.4577 - val_mae: 1.6729\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 6.3283 - mae: 1.8435 - val_loss: 4.4613 - val_mae: 1.6970\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.4389 - mae: 1.8568 - val_loss: 4.0556 - val_mae: 1.5603\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.1798 - mae: 1.8025 - val_loss: 4.0045 - val_mae: 1.5635\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.4636 - mae: 1.8450 - val_loss: 3.9816 - val_mae: 1.5697\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.3597 - mae: 1.8045 - val_loss: 4.0632 - val_mae: 1.5781\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.2482 - mae: 1.7977 - val_loss: 3.8423 - val_mae: 1.5083\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.5127 - mae: 1.8283 - val_loss: 4.8160 - val_mae: 1.8435\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 6.8746 - mae: 1.8999 - val_loss: 4.5031 - val_mae: 1.7334\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 6.5486 - mae: 1.9099 - val_loss: 5.6605 - val_mae: 1.8043\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 7.4399 - mae: 2.0064 - val_loss: 4.0916 - val_mae: 1.5813\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.0778 - mae: 1.7906 - val_loss: 4.0279 - val_mae: 1.5829\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 6.3370 - mae: 1.8257 - val_loss: 3.8557 - val_mae: 1.5405\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 6.3032 - mae: 1.8090 - val_loss: 4.3134 - val_mae: 1.6688\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.0881 - mae: 1.7900 - val_loss: 3.8306 - val_mae: 1.5125\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.0963 - mae: 1.7910 - val_loss: 4.0243 - val_mae: 1.5861\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.1314 - mae: 1.8076 - val_loss: 4.1300 - val_mae: 1.6202\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.4219 - mae: 1.8335 - val_loss: 4.6172 - val_mae: 1.7717\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.2039 - mae: 1.8255 - val_loss: 4.4206 - val_mae: 1.6320\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.4910 - mae: 1.8608 - val_loss: 3.9485 - val_mae: 1.5451\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.3515 - mae: 1.8390 - val_loss: 4.4802 - val_mae: 1.7486\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.9939 - mae: 1.7945 - val_loss: 4.0604 - val_mae: 1.5961\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.0959 - mae: 1.7852 - val_loss: 3.9815 - val_mae: 1.5696\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.0038 - mae: 1.7752 - val_loss: 3.8191 - val_mae: 1.5053\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.1388 - mae: 1.7852 - val_loss: 4.0926 - val_mae: 1.6279\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.8996 - mae: 1.7575 - val_loss: 3.9375 - val_mae: 1.5539\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.8814 - mae: 1.7524 - val_loss: 3.9402 - val_mae: 1.5388\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.9551 - mae: 1.7592 - val_loss: 3.9893 - val_mae: 1.5927\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.0924 - mae: 1.7756 - val_loss: 3.8696 - val_mae: 1.5329\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.8495 - mae: 1.7489 - val_loss: 4.1795 - val_mae: 1.6385\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.9006 - mae: 1.7758 - val_loss: 4.4488 - val_mae: 1.7224\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 6.1540 - mae: 1.8012 - val_loss: 3.7460 - val_mae: 1.5376\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 6.0440 - mae: 1.7704 - val_loss: 3.9798 - val_mae: 1.5933\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 5.9244 - mae: 1.7662 - val_loss: 4.2627 - val_mae: 1.6639\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 5.8295 - mae: 1.7521 - val_loss: 3.7714 - val_mae: 1.5466\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 6.1092 - mae: 1.7788 - val_loss: 3.9563 - val_mae: 1.5809\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 5.8806 - mae: 1.7766 - val_loss: 4.5413 - val_mae: 1.7480\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 5.8379 - mae: 1.7456 - val_loss: 3.8881 - val_mae: 1.5677\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.8079 - mae: 1.7441 - val_loss: 4.0128 - val_mae: 1.5814\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.7959 - mae: 1.7409 - val_loss: 4.2217 - val_mae: 1.6404\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.8244 - mae: 1.7700 - val_loss: 4.4620 - val_mae: 1.7327\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.9045 - mae: 1.7793 - val_loss: 4.2540 - val_mae: 1.6556\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 5.8436 - mae: 1.7719 - val_loss: 3.9829 - val_mae: 1.5836\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 5.8737 - mae: 1.7551 - val_loss: 4.0325 - val_mae: 1.5464\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.6688 - mae: 1.7262 - val_loss: 4.1785 - val_mae: 1.6206\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.7086 - mae: 1.7335 - val_loss: 4.1714 - val_mae: 1.6227\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.9059 - mae: 1.7477 - val_loss: 4.1063 - val_mae: 1.6133\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.7152 - mae: 1.7328 - val_loss: 4.2515 - val_mae: 1.6540\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.9591 - mae: 1.7941 - val_loss: 5.2081 - val_mae: 1.8784\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.7621 - mae: 1.7811 - val_loss: 3.9581 - val_mae: 1.5745\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.9201 - mae: 1.7673 - val_loss: 3.9938 - val_mae: 1.5780\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.8389 - mae: 1.7412 - val_loss: 4.4339 - val_mae: 1.6894\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 6.1144 - mae: 1.8329 - val_loss: 5.3891 - val_mae: 1.9485\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.2161 - mae: 1.8038 - val_loss: 4.3342 - val_mae: 1.6847\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.8902 - mae: 1.7669 - val_loss: 4.3754 - val_mae: 1.6656\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.0021 - mae: 1.7964 - val_loss: 4.0686 - val_mae: 1.6174\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.7399 - mae: 1.7415 - val_loss: 4.4365 - val_mae: 1.7266\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.7655 - mae: 1.7534 - val_loss: 4.4222 - val_mae: 1.6990\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.6218 - mae: 1.7377 - val_loss: 3.9899 - val_mae: 1.6106\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.5927 - mae: 1.7352 - val_loss: 4.7721 - val_mae: 1.8127\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 7ms/step - loss: 5.8718 - mae: 1.7691 - val_loss: 3.9726 - val_mae: 1.5768\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.6033 - mae: 1.7251 - val_loss: 4.0593 - val_mae: 1.5983\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.6013 - mae: 1.7102 - val_loss: 4.0528 - val_mae: 1.6284\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.7214 - mae: 1.7632 - val_loss: 4.6048 - val_mae: 1.7465\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.5967 - mae: 1.7171 - val_loss: 4.0860 - val_mae: 1.6097\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.6574 - mae: 1.7371 - val_loss: 4.1570 - val_mae: 1.6369\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.5731 - mae: 1.7261 - val_loss: 4.1627 - val_mae: 1.6149\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.5641 - mae: 1.7168 - val_loss: 4.0894 - val_mae: 1.6301\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.7659 - mae: 1.7300 - val_loss: 4.3935 - val_mae: 1.6796\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.7177 - mae: 1.7414 - val_loss: 4.1752 - val_mae: 1.6322\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.5739 - mae: 1.7127 - val_loss: 4.1393 - val_mae: 1.6403\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.7601 - mae: 1.7410 - val_loss: 5.2644 - val_mae: 1.9155\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.8328 - mae: 1.7667 - val_loss: 4.4096 - val_mae: 1.6663\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.5058 - mae: 1.7039 - val_loss: 4.1922 - val_mae: 1.6299\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.5443 - mae: 1.7017 - val_loss: 4.1597 - val_mae: 1.6366\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.4998 - mae: 1.6995 - val_loss: 4.2596 - val_mae: 1.6480\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.5082 - mae: 1.7104 - val_loss: 4.1536 - val_mae: 1.6301\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.0536 - mae: 1.8077 - val_loss: 4.7074 - val_mae: 1.7792\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.5868 - mae: 1.7135 - val_loss: 4.2601 - val_mae: 1.6665\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.6554 - mae: 1.7452 - val_loss: 4.2402 - val_mae: 1.6258\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.5060 - mae: 1.7044 - val_loss: 3.8107 - val_mae: 1.5416\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.6501 - mae: 1.7493 - val_loss: 4.4538 - val_mae: 1.6998\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.7286 - mae: 1.7396 - val_loss: 4.1665 - val_mae: 1.6361\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.4697 - mae: 1.6718 - val_loss: 4.4584 - val_mae: 1.7054\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.4868 - mae: 1.6943 - val_loss: 4.1014 - val_mae: 1.6111\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.4754 - mae: 1.7225 - val_loss: 4.2817 - val_mae: 1.6723\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.4601 - mae: 1.7126 - val_loss: 4.5249 - val_mae: 1.7030\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.3539 - mae: 1.7011 - val_loss: 4.1763 - val_mae: 1.6319\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.4389 - mae: 1.6777 - val_loss: 4.1612 - val_mae: 1.6461\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.4834 - mae: 1.7130 - val_loss: 4.1617 - val_mae: 1.6035\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.3052 - mae: 1.6761 - val_loss: 4.0999 - val_mae: 1.6293\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.4262 - mae: 1.6822 - val_loss: 4.2266 - val_mae: 1.6739\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.3819 - mae: 1.6965 - val_loss: 4.3753 - val_mae: 1.6559\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.3869 - mae: 1.6926 - val_loss: 4.3155 - val_mae: 1.6656\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.2590 - mae: 1.6603 - val_loss: 3.9615 - val_mae: 1.5916\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.3842 - mae: 1.6900 - val_loss: 3.9623 - val_mae: 1.5786\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.3228 - mae: 1.6919 - val_loss: 4.7455 - val_mae: 1.7418\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.3680 - mae: 1.6901 - val_loss: 4.1485 - val_mae: 1.6381\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.05, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzY8zjL5s9jS",
    "outputId": "48a88cbe-ef1d-456a-b104-f0210b2bb424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 15.6829 - mae: 2.4086\n",
      "Mean squared error on test data:  15.682909965515137\n",
      "Mean absolute error on test data:  2.4086341857910156\n"
     ]
    }
   ],
   "source": [
    "mse_nn, mae_nn = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Mean squared error on test data: ', mse_nn)\n",
    "print('Mean absolute error on test data: ', mae_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
